{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing a Training Set and Training a Decision Tree\n",
    "This is a hands-on task where we build a predictive model using Decision Trees discussed in class. For this part, we will be using the data in `cell2cell_data.csv`.\n",
    "\n",
    "These historical data consist of 39,859 customers: 19,901 customers that churned (i.e., left the company) and 19,958 that did not churn (see the `\"churndep\"` variable). Here are the data set's 11 possible predictor variables for churning behavior: \n",
    "\n",
    "```\n",
    "Pos.  Var. Name  Var. Description\n",
    "----- ---------- --------------------------------------------------------------\n",
    "1     revenue    Mean monthly revenue in dollars\n",
    "2     outcalls   Mean number of outbound voice calls\n",
    "3     incalls    Mean number of inbound voice calls\n",
    "4     months     Months in Service\n",
    "5     eqpdays    Number of days the customer has had his/her current equipment\n",
    "6     webcap     Handset is web capable\n",
    "7     marryyes   Married (1=Yes; 0=No)\n",
    "8     travel     Has traveled to non-US country (1=Yes; 0=No)\n",
    "9     pcown      Owns a personal computer (1=Yes; 0=No)\n",
    "10    creditcd   Possesses a credit card (1=Yes; 0=No)\n",
    "11    retcalls   Number of calls previously made to retention team\n",
    "```\n",
    "\n",
    "The 12th column, the target variable `\"churndep\"`, equals 1 if the customer churned, and 0 otherwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Load the data and prepare it for modeling. Note that the features are already processed for you, so the only thing needed here is split the data into training and testing. Use pandas to create two data frames: train_df and test_df, where train_df has 80% of the data chosen uniformly at random without replacement (test_df should have the other 20%). Also, make sure to write your own code to do the splits. You may use any random() function numpy but DO NOT use the data splitting functions from Sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>revenue</th>\n",
       "      <th>outcalls</th>\n",
       "      <th>incalls</th>\n",
       "      <th>months</th>\n",
       "      <th>eqpdays</th>\n",
       "      <th>webcap</th>\n",
       "      <th>maryyes</th>\n",
       "      <th>travel</th>\n",
       "      <th>pcown</th>\n",
       "      <th>creditcd</th>\n",
       "      <th>retcalls</th>\n",
       "      <th>churndep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16596</th>\n",
       "      <td>136.47</td>\n",
       "      <td>85.33</td>\n",
       "      <td>23.00</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37968</th>\n",
       "      <td>96.96</td>\n",
       "      <td>37.67</td>\n",
       "      <td>75.00</td>\n",
       "      <td>18</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36313</th>\n",
       "      <td>14.21</td>\n",
       "      <td>36.00</td>\n",
       "      <td>16.67</td>\n",
       "      <td>18</td>\n",
       "      <td>254</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8434</th>\n",
       "      <td>50.24</td>\n",
       "      <td>36.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>10</td>\n",
       "      <td>316</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9954</th>\n",
       "      <td>35.19</td>\n",
       "      <td>23.00</td>\n",
       "      <td>5.67</td>\n",
       "      <td>16</td>\n",
       "      <td>472</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5058</th>\n",
       "      <td>37.48</td>\n",
       "      <td>4.67</td>\n",
       "      <td>5.00</td>\n",
       "      <td>24</td>\n",
       "      <td>720</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1570</th>\n",
       "      <td>79.22</td>\n",
       "      <td>20.67</td>\n",
       "      <td>22.33</td>\n",
       "      <td>30</td>\n",
       "      <td>909</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30177</th>\n",
       "      <td>59.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10</td>\n",
       "      <td>296</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9666</th>\n",
       "      <td>34.99</td>\n",
       "      <td>15.67</td>\n",
       "      <td>2.67</td>\n",
       "      <td>11</td>\n",
       "      <td>316</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34529</th>\n",
       "      <td>65.53</td>\n",
       "      <td>12.67</td>\n",
       "      <td>6.00</td>\n",
       "      <td>7</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15104</th>\n",
       "      <td>86.24</td>\n",
       "      <td>38.67</td>\n",
       "      <td>12.33</td>\n",
       "      <td>7</td>\n",
       "      <td>204</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24388</th>\n",
       "      <td>14.96</td>\n",
       "      <td>21.67</td>\n",
       "      <td>14.33</td>\n",
       "      <td>18</td>\n",
       "      <td>547</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39660</th>\n",
       "      <td>47.07</td>\n",
       "      <td>25.00</td>\n",
       "      <td>43.33</td>\n",
       "      <td>32</td>\n",
       "      <td>109</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24952</th>\n",
       "      <td>38.90</td>\n",
       "      <td>7.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13</td>\n",
       "      <td>401</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15951</th>\n",
       "      <td>84.32</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28185</th>\n",
       "      <td>70.77</td>\n",
       "      <td>6.67</td>\n",
       "      <td>3.00</td>\n",
       "      <td>7</td>\n",
       "      <td>207</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2609</th>\n",
       "      <td>90.30</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>20</td>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15109</th>\n",
       "      <td>83.83</td>\n",
       "      <td>57.67</td>\n",
       "      <td>4.33</td>\n",
       "      <td>51</td>\n",
       "      <td>638</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32004</th>\n",
       "      <td>80.12</td>\n",
       "      <td>115.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>13</td>\n",
       "      <td>216</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29156</th>\n",
       "      <td>79.28</td>\n",
       "      <td>69.67</td>\n",
       "      <td>39.33</td>\n",
       "      <td>12</td>\n",
       "      <td>335</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>39.97</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16</td>\n",
       "      <td>330</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19077</th>\n",
       "      <td>90.23</td>\n",
       "      <td>83.33</td>\n",
       "      <td>51.00</td>\n",
       "      <td>14</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37772</th>\n",
       "      <td>81.75</td>\n",
       "      <td>59.00</td>\n",
       "      <td>31.67</td>\n",
       "      <td>21</td>\n",
       "      <td>170</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10347</th>\n",
       "      <td>37.33</td>\n",
       "      <td>4.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>32</td>\n",
       "      <td>608</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29906</th>\n",
       "      <td>31.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12</td>\n",
       "      <td>354</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25512</th>\n",
       "      <td>53.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12</td>\n",
       "      <td>354</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37299</th>\n",
       "      <td>52.89</td>\n",
       "      <td>48.00</td>\n",
       "      <td>3.67</td>\n",
       "      <td>19</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12251</th>\n",
       "      <td>34.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12</td>\n",
       "      <td>357</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37613</th>\n",
       "      <td>73.53</td>\n",
       "      <td>10.67</td>\n",
       "      <td>9.00</td>\n",
       "      <td>27</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14240</th>\n",
       "      <td>19.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10</td>\n",
       "      <td>302</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36312</th>\n",
       "      <td>70.14</td>\n",
       "      <td>58.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>37</td>\n",
       "      <td>264</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25412</th>\n",
       "      <td>107.16</td>\n",
       "      <td>51.33</td>\n",
       "      <td>20.67</td>\n",
       "      <td>12</td>\n",
       "      <td>362</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26415</th>\n",
       "      <td>30.25</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12</td>\n",
       "      <td>347</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3843</th>\n",
       "      <td>54.79</td>\n",
       "      <td>6.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>21</td>\n",
       "      <td>628</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25479</th>\n",
       "      <td>82.93</td>\n",
       "      <td>33.67</td>\n",
       "      <td>27.00</td>\n",
       "      <td>20</td>\n",
       "      <td>591</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28047</th>\n",
       "      <td>64.93</td>\n",
       "      <td>10.33</td>\n",
       "      <td>5.00</td>\n",
       "      <td>7</td>\n",
       "      <td>223</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3638</th>\n",
       "      <td>31.85</td>\n",
       "      <td>17.33</td>\n",
       "      <td>6.33</td>\n",
       "      <td>23</td>\n",
       "      <td>707</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37390</th>\n",
       "      <td>35.51</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28846</th>\n",
       "      <td>50.79</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7</td>\n",
       "      <td>202</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12105</th>\n",
       "      <td>40.98</td>\n",
       "      <td>28.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>18</td>\n",
       "      <td>416</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34008</th>\n",
       "      <td>59.78</td>\n",
       "      <td>4.67</td>\n",
       "      <td>1.00</td>\n",
       "      <td>12</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27406</th>\n",
       "      <td>74.36</td>\n",
       "      <td>91.33</td>\n",
       "      <td>27.00</td>\n",
       "      <td>17</td>\n",
       "      <td>505</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1868</th>\n",
       "      <td>79.28</td>\n",
       "      <td>11.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>30</td>\n",
       "      <td>901</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35054</th>\n",
       "      <td>64.14</td>\n",
       "      <td>80.67</td>\n",
       "      <td>44.00</td>\n",
       "      <td>8</td>\n",
       "      <td>247</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5252</th>\n",
       "      <td>29.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14</td>\n",
       "      <td>406</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32518</th>\n",
       "      <td>125.61</td>\n",
       "      <td>50.67</td>\n",
       "      <td>32.67</td>\n",
       "      <td>8</td>\n",
       "      <td>243</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23361</th>\n",
       "      <td>37.25</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>17</td>\n",
       "      <td>531</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8150</th>\n",
       "      <td>43.72</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17</td>\n",
       "      <td>503</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15149</th>\n",
       "      <td>34.28</td>\n",
       "      <td>13.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>8</td>\n",
       "      <td>232</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27392</th>\n",
       "      <td>29.99</td>\n",
       "      <td>6.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15</td>\n",
       "      <td>451</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24655</th>\n",
       "      <td>37.71</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0.33</td>\n",
       "      <td>20</td>\n",
       "      <td>591</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13968</th>\n",
       "      <td>21.37</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11</td>\n",
       "      <td>320</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36062</th>\n",
       "      <td>99.11</td>\n",
       "      <td>22.33</td>\n",
       "      <td>1.67</td>\n",
       "      <td>21</td>\n",
       "      <td>161</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15597</th>\n",
       "      <td>31.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.67</td>\n",
       "      <td>7</td>\n",
       "      <td>205</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21171</th>\n",
       "      <td>50.83</td>\n",
       "      <td>28.33</td>\n",
       "      <td>3.33</td>\n",
       "      <td>20</td>\n",
       "      <td>580</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17750</th>\n",
       "      <td>53.25</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>36</td>\n",
       "      <td>244</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14554</th>\n",
       "      <td>71.04</td>\n",
       "      <td>16.33</td>\n",
       "      <td>1.33</td>\n",
       "      <td>8</td>\n",
       "      <td>219</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36950</th>\n",
       "      <td>45.17</td>\n",
       "      <td>17.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>53</td>\n",
       "      <td>563</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22326</th>\n",
       "      <td>62.31</td>\n",
       "      <td>24.33</td>\n",
       "      <td>5.00</td>\n",
       "      <td>22</td>\n",
       "      <td>647</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10937</th>\n",
       "      <td>19.34</td>\n",
       "      <td>11.33</td>\n",
       "      <td>1.33</td>\n",
       "      <td>14</td>\n",
       "      <td>429</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31887 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       revenue  outcalls  incalls  months  eqpdays  webcap  maryyes  travel  \\\n",
       "16596   136.47     85.33    23.00       9       21       1        0       0   \n",
       "37968    96.96     37.67    75.00      18      136       1        0       0   \n",
       "36313    14.21     36.00    16.67      18      254       1        1       0   \n",
       "8434     50.24     36.00     1.00      10      316       1        0       0   \n",
       "9954     35.19     23.00     5.67      16      472       1        0       0   \n",
       "5058     37.48      4.67     5.00      24      720       1        1       0   \n",
       "1570     79.22     20.67    22.33      30      909       0        0       0   \n",
       "30177    59.31      0.00     0.00      10      296       1        1       1   \n",
       "9666     34.99     15.67     2.67      11      316       1        0       0   \n",
       "34529    65.53     12.67     6.00       7       45       1        0       0   \n",
       "15104    86.24     38.67    12.33       7      204       1        1       0   \n",
       "24388    14.96     21.67    14.33      18      547       1        0       0   \n",
       "39660    47.07     25.00    43.33      32      109       1        0       0   \n",
       "24952    38.90      7.33     0.00      13      401       1        0       0   \n",
       "15951    84.32      5.00     0.00      14      200       1        0       0   \n",
       "28185    70.77      6.67     3.00       7      207       1        0       0   \n",
       "2609     90.30      1.33     0.33      20       73       1        0       0   \n",
       "15109    83.83     57.67     4.33      51      638       1        1       0   \n",
       "32004    80.12    115.00    20.00      13      216       1        0       0   \n",
       "29156    79.28     69.67    39.33      12      335       1        0       0   \n",
       "672      39.97      3.67     0.00      16      330       1        0       0   \n",
       "19077    90.23     83.33    51.00      14      110       1        0       0   \n",
       "37772    81.75     59.00    31.67      21      170       1        0       0   \n",
       "10347    37.33      4.33     0.00      32      608       1        1       0   \n",
       "29906    31.23      0.00     0.00      12      354       1        1       0   \n",
       "25512    53.44      0.00     0.00      12      354       1        0       0   \n",
       "37299    52.89     48.00     3.67      19       23       1        0       0   \n",
       "12251    34.24      0.00     0.00      12      357       1        0       0   \n",
       "37613    73.53     10.67     9.00      27       80       1        0       0   \n",
       "14240    19.99      0.00     0.00      10      302       1        1       0   \n",
       "...        ...       ...      ...     ...      ...     ...      ...     ...   \n",
       "36312    70.14     58.00     9.00      37      264       1        0       0   \n",
       "25412   107.16     51.33    20.67      12      362       1        0       0   \n",
       "26415    30.25      0.67     0.00      12      347       1        0       0   \n",
       "3843     54.79      6.33     0.33      21      628       1        1       0   \n",
       "25479    82.93     33.67    27.00      20      591       1        0       0   \n",
       "28047    64.93     10.33     5.00       7      223       1        0       0   \n",
       "3638     31.85     17.33     6.33      23      707       1        0       0   \n",
       "37390    35.51      3.67     0.00       7        3       1        0       0   \n",
       "28846    50.79      3.67     0.00       7      202       1        0       0   \n",
       "12105    40.98     28.00     0.33      18      416       1        0       0   \n",
       "34008    59.78      4.67     1.00      12      110       1        0       0   \n",
       "27406    74.36     91.33    27.00      17      505       1        1       0   \n",
       "1868     79.28     11.00     4.00      30      901       1        0       0   \n",
       "35054    64.14     80.67    44.00       8      247       1        0       0   \n",
       "5252     29.99      0.00     0.00      14      406       0        1       0   \n",
       "32518   125.61     50.67    32.67       8      243       1        1       0   \n",
       "23361    37.25      4.00     1.00      17      531       1        0       0   \n",
       "8150     43.72      2.00     0.00      17      503       1        0       0   \n",
       "15149    34.28     13.00     0.33       8      232       1        0       0   \n",
       "27392    29.99      6.33     0.00      15      451       1        1       0   \n",
       "24655    37.71      3.67     0.33      20      591       1        1       0   \n",
       "13968    21.37      1.33     0.00      11      320       1        1       0   \n",
       "36062    99.11     22.33     1.67      21      161       1        1       0   \n",
       "15597    31.11      0.00     0.67       7      205       1        1       0   \n",
       "21171    50.83     28.33     3.33      20      580       0        0       0   \n",
       "17750    53.25      2.33     0.33      36      244       1        0       0   \n",
       "14554    71.04     16.33     1.33       8      219       1        0       0   \n",
       "36950    45.17     17.00    22.00      53      563       1        0       1   \n",
       "22326    62.31     24.33     5.00      22      647       1        0       0   \n",
       "10937    19.34     11.33     1.33      14      429       1        1       0   \n",
       "\n",
       "       pcown  creditcd  retcalls  churndep  \n",
       "16596      0         0         0         1  \n",
       "37968      0         0         0         0  \n",
       "36313      0         1         0         0  \n",
       "8434       0         0         0         1  \n",
       "9954       0         1         0         1  \n",
       "5058       1         1         0         1  \n",
       "1570       0         1         0         1  \n",
       "30177      1         1         0         0  \n",
       "9666       0         0         0         1  \n",
       "34529      0         0         0         0  \n",
       "15104      0         1         0         1  \n",
       "24388      0         0         0         0  \n",
       "39660      0         1         0         0  \n",
       "24952      0         0         0         0  \n",
       "15951      0         1         0         1  \n",
       "28185      0         0         0         0  \n",
       "2609       0         1         1         1  \n",
       "15109      1         1         0         1  \n",
       "32004      0         0         0         0  \n",
       "29156      0         0         0         0  \n",
       "672        0         0         1         1  \n",
       "19077      0         1         0         1  \n",
       "37772      1         1         0         0  \n",
       "10347      0         1         0         1  \n",
       "29906      0         1         0         0  \n",
       "25512      0         0         0         0  \n",
       "37299      0         0         0         0  \n",
       "12251      1         1         0         1  \n",
       "37613      0         0         0         0  \n",
       "14240      1         1         0         1  \n",
       "...      ...       ...       ...       ...  \n",
       "36312      0         0         0         0  \n",
       "25412      0         0         0         0  \n",
       "26415      0         0         0         0  \n",
       "3843       1         0         0         1  \n",
       "25479      0         1         0         0  \n",
       "28047      0         0         0         0  \n",
       "3638       0         0         0         1  \n",
       "37390      0         1         0         0  \n",
       "28846      0         0         0         0  \n",
       "12105      0         1         0         1  \n",
       "34008      0         0         0         0  \n",
       "27406      0         1         0         0  \n",
       "1868       0         0         0         1  \n",
       "35054      0         1         0         0  \n",
       "5252       0         1         0         1  \n",
       "32518      1         1         0         0  \n",
       "23361      0         0         0         0  \n",
       "8150       0         1         0         1  \n",
       "15149      0         1         0         1  \n",
       "27392      0         1         0         0  \n",
       "24655      1         1         0         0  \n",
       "13968      1         1         0         1  \n",
       "36062      0         1         0         0  \n",
       "15597      0         1         0         1  \n",
       "21171      0         0         0         0  \n",
       "17750      0         0         0         1  \n",
       "14554      0         1         0         1  \n",
       "36950      0         1         0         0  \n",
       "22326      0         0         0         0  \n",
       "10937      0         1         0         1  \n",
       "\n",
       "[31887 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "main_df = pd.DataFrame(pd.read_csv('/Users/derin/Desktop/1001Data/cell2cell_data.csv'))\n",
    "main_df.columns = ['revenue', 'outcalls', 'incalls', 'months',\n",
    "                   'eqpdays', 'webcap', 'maryyes', 'travel', 'pcown',\n",
    "                  'creditcd', 'retcalls', 'churndep']\n",
    "train_df = main_df.sample(axis=0, frac=0.8, replace=False)\n",
    "test_df = main_df.drop(train_df.index)\n",
    "\n",
    "train_df\n",
    "# test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. If we had to, how would we prove to ourselves or a colleague that our data was indeed randomly sampled on X? And by prove, I mean empirically, not just showing this person our code. Don't actually do the work, just describe in your own words a test you could here. Hint: think about this in terms of selection bias and use notes from our 2nd lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__: \n",
    "\n",
    "In our datasets, train_df and test_df, each instance is characterized by a set of features: ['revenue', 'outcalls', 'incalls','months','eqpdays', 'webcap', 'maryyes', 'travel', 'pcown','creditcd', 'retcalls', 'churndep'].\n",
    "\n",
    "If being in one of the samples is independent of the features listed above, we can say that the\n",
    "sample is unbiased. Therefore, we need to come up with a way/ways to show that there aren't any significant differences between the column descriptions of the same feature in different datasets. For example showing that the mean values for 'revenue' aren't significantly different than each other and the frequency values of training set's 'marryyes' and test set's 'creditcd' aren't significantly different than each other is necessary. \n",
    "\n",
    "For 'revenue', 'outcalls', 'incalls', 'months', 'eqpdays', it makes sense to compare the mean values. That is, we need to see if that feature's mean values are similar to each other in different datasets. For this purpose, I would use two-tailed independent samples t test.\n",
    "\n",
    "__Two-Sample t-test Hypotheses__\n",
    "\n",
    "$H_{0}: \\mu_{Training} = \\mu_{Test}$, In words, there is not a significant difference between the mean values of the two samples with respect to [feature].\n",
    "\n",
    "$H_{1}: \\mu_{Training} \\neq \\mu_{Test}$, In words, there is a significant difference between the mean values of the two samples with respect to [feature].\n",
    "\n",
    "In the case of __'revenue'__, the code will look like this: \n",
    "\n",
    "    rev_train = np.array(train_df['revenue'])\n",
    "\n",
    "    rev_test = np.array(test_df['revenue'])\n",
    "\n",
    "    scipy.stats.ttest_ind(rev_train, rev_test)\n",
    "\n",
    "And the output will look like this: \n",
    "\n",
    "    Ttest_indResult(statistic=-0.90312554984292426, pvalue=0.36646472342202585). \n",
    "\n",
    "This pvalue, well above 0.05 default, tells us that the difference is not significant between two data sets. Another way to verify this is by looking at the absolute value of the test statictic, here 0.9 < 1.96, that is lower than the treshold. This again tells us that the difference is not significant between the two data sets. \n",
    "\n",
    "For the dummy variables 'webcap', 'maryyes', 'travel', 'pcown','creditcd', and 'churndep', it makes more sense to compare the frequency distributions. That is, testing if the 1s and 0s are distributed similarly in each sample's feature array. \n",
    "\n",
    "Because our two datasets are unpaired and the variables are binomial, I would use Fisher's Exact Test to determine whether there are significant differences in terms of frequency between samples. \n",
    "\n",
    "Here I excluded 'retcalls' from the test because it is the number of calls made to retention team, which is almost always zero for all customers in both samples. Fisher's Exact Test works for 2x2 tables, so it might not always yield the expected result as its frequency array is often 1-dimensiomnal. \n",
    "\n",
    "__Fisher's Exact Test Hypotheses__\n",
    "\n",
    "$H_{0}$: There is not an association between the frequency values of [feature] across samples. \n",
    "\n",
    "$H_{1}$: There is an association between the frequency values of [feature] across samples. \n",
    "\n",
    "In the case of __'creditcd'__, the code will look like: \n",
    "\n",
    "    crcd_tr_fr = np.unique(np.array(train_df['creditcd']), return_counts=True)\n",
    "\n",
    "    crcd_ts_fr = np.unique(np.array(test_df['creditcd']), return_counts=True)\n",
    "\n",
    "    scipy.stats.fisher_exact(np.array([crcd_tr_fr[1], crcd_ts_fr[1]]))\n",
    "\n",
    "And the output will look like this: \n",
    "\n",
    "    (0.98702506577266447, 0.63010982513493607) \n",
    "    \n",
    "which tells us that the observed distribution or imbalance in frequency distributions across samples is not significant since 0.63 > 0.05. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=0.84942359428310732, pvalue=0.3956507309454842)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.98280852276216923, 0.52081023783949232)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## To see if all of them work, this part can be skipped. \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.stats import itemfreq\n",
    "\n",
    "## Revenue\n",
    "rev_train = np.array(train_df['revenue'])\n",
    "rev_test = np.array(test_df['revenue'])\n",
    "##  Eqpdays\n",
    "eqp_train = np.array(train_df['eqpdays'])\n",
    "eqp_test = np.array(test_df['eqpdays'])\n",
    "## Incalls\n",
    "inc_train = np.array(train_df['incalls'])\n",
    "inc_test = np.array(test_df['incalls'])\n",
    "## Outcalls\n",
    "out_train = np.array(train_df['outcalls'])\n",
    "out_test = np.array(test_df['outcalls'])\n",
    "## Months\n",
    "mon_train = np.array(train_df['months'])\n",
    "mon_test = np.array(test_df['months'])\n",
    "\n",
    "print (scipy.stats.ttest_ind(rev_train, rev_test))\n",
    "\n",
    "## BINARY VARIABLE FREQUENCIES / Frequencies for Fisher's Exact\n",
    "## tr_fr: train_df frequency\n",
    "## ts_fr: test_df frequency\n",
    "\n",
    "web_tr_fr = np.unique(np.array(train_df['webcap']), return_counts=True) \n",
    "web_ts_fr = np.unique(np.array(test_df['webcap']), return_counts=True)\n",
    "\n",
    "marryyes_tr_fr = np.unique(np.array(train_df['maryyes']), return_counts=True)\n",
    "marryyes_ts_fr = np.unique(np.array(test_df['maryyes']), return_counts=True)\n",
    "\n",
    "travel_tr_fr = np.unique(np.array(train_df['travel']), return_counts=True)\n",
    "travel_ts_fr = np.unique(np.array(test_df['travel']), return_counts=True)\n",
    "\n",
    "pcown_tr_fr = np.unique(np.array(train_df['pcown']), return_counts=True)\n",
    "pcown_ts_fr = np.unique(np.array(test_df['pcown']), return_counts=True)\n",
    "\n",
    "crcd_tr_fr = np.unique(np.array(train_df['creditcd']), return_counts=True)\n",
    "crcd_ts_fr = np.unique(np.array(test_df['creditcd']), return_counts=True)\n",
    "\n",
    "ret_tr_fr = np.unique(np.array(train_df['retcalls']), return_counts=True)\n",
    "ret_ts_fr = np.unique(np.array(test_df['retcalls']), return_counts=True)\n",
    "\n",
    "churn_tr_fr = np.unique(np.array(train_df['churndep']), return_counts=True)\n",
    "churn_ts_fr = np.unique(np.array(test_df['churndep']), return_counts=True)\n",
    "\n",
    "\n",
    "scipy.stats.fisher_exact(np.array([crcd_tr_fr[1], crcd_ts_fr[1]]))\n",
    "\n",
    "# scipy.stats.fisher_exact(np.array([churn_tr_fr[1], churn_ts_fr[1]]))\n",
    "# scipy.stats.fisher_exact(np.array([web_tr_fr[1], web_ts_fr[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Now build and train a decision tree classifier using `DecisionTreeClassifier()` [(manual page)](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) on train_df to predict the `\"churndep\"` target variable. Make sure to use `criterion='entropy'` when instantiating an instance of `DecisionTreeClassifier()`. For all other settings you should use all of the default options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "revenue     0.267046\n",
       "outcalls    0.191215\n",
       "incalls     0.124128\n",
       "months      0.082994\n",
       "eqpdays     0.250049\n",
       "webcap      0.003944\n",
       "maryyes     0.023604\n",
       "travel      0.011783\n",
       "pcown       0.019845\n",
       "creditcd    0.020472\n",
       "retcalls    0.004918\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn import tree\n",
    "from scipy.stats import entropy\n",
    "\n",
    "Y = train_df['churndep']            ## setting the target variable to 'churndep' of training data\n",
    "X = train_df.drop('churndep', 1)    ## setting the remaining variables of the training data as predictors\n",
    "\n",
    "churn_tree = sklearn.tree.DecisionTreeClassifier(criterion='entropy')\n",
    "churn_tree.fit(X, Y)\n",
    "\n",
    "feature_mi = churn_tree.feature_importances_\n",
    "M = pd.Series(feature_mi, index = X.columns)\n",
    "M_dict = dict(M)\n",
    "M\n",
    "# M_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Using the resulting model from 2.2, show a bar plot of feature names and their feature importance (hint: check the attributes of the `DecisionTreeClassifier()` object directly in IPython or check the manual!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x11a29f160>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIgnChgiIuLJ/wc5qAs6a5w4XwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1141a6c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "%matplotlib inline\n",
    "\n",
    "N = list(M_dict)\n",
    "Q = '\\n'.join(N)\n",
    "\n",
    "y = range(len(feature_mi))\n",
    "plt.bar(y, M_dict.values(), width=1/1.5, color = 'darkmagenta')\n",
    "plt.xlabel('Features') \n",
    "plt.ylabel('MI')\n",
    "legend_label = mpatches.Patch(color='darkmagenta', label=Q)  ## Q = '\\n'.join(N)\n",
    "plt.legend(handles=[legend_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Is the relationship between the top 3 most important features (as measured here) negative or positive? If your marketing director asked you to explain the top 3 drivers of churn, how would you interpret the relationship between these 3 features and the churn outcome?  What \"real-life\" connection can you draw between each variable and churn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>revenue</th>\n",
       "      <th>months</th>\n",
       "      <th>outcalls</th>\n",
       "      <th>eqpdays</th>\n",
       "      <th>churndep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>revenue</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.015664</td>\n",
       "      <td>0.503045</td>\n",
       "      <td>-0.219314</td>\n",
       "      <td>-0.016766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>months</th>\n",
       "      <td>-0.015664</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.041273</td>\n",
       "      <td>0.486769</td>\n",
       "      <td>0.024605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outcalls</th>\n",
       "      <td>0.503045</td>\n",
       "      <td>-0.041273</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.240228</td>\n",
       "      <td>-0.037780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eqpdays</th>\n",
       "      <td>-0.219314</td>\n",
       "      <td>0.486769</td>\n",
       "      <td>-0.240228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.116396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>churndep</th>\n",
       "      <td>-0.016766</td>\n",
       "      <td>0.024605</td>\n",
       "      <td>-0.037780</td>\n",
       "      <td>0.116396</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           revenue    months  outcalls   eqpdays  churndep\n",
       "revenue   1.000000 -0.015664  0.503045 -0.219314 -0.016766\n",
       "months   -0.015664  1.000000 -0.041273  0.486769  0.024605\n",
       "outcalls  0.503045 -0.041273  1.000000 -0.240228 -0.037780\n",
       "eqpdays  -0.219314  0.486769 -0.240228  1.000000  0.116396\n",
       "churndep -0.016766  0.024605 -0.037780  0.116396  1.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_mi3_df = pd.DataFrame([train_df['revenue'], train_df['months'], train_df['outcalls'], \n",
    "                             train_df['eqpdays'], train_df['churndep']])\n",
    "feature_mi3_df = feature_mi3_df.T\n",
    "feature_mi3_df.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__  \n",
    "\n",
    "The three most important features are revenue, eqpdays, and outcalls. \n",
    "\n",
    "According to the pearson correlation table above:\n",
    "\n",
    "1 - The linear relationship between mean monthly revenue and churn is negative, even though the correlation coefficient is relatively small. \n",
    "\n",
    "    The main reason that the customers who pay more are less likely to churn is that they tend to pay more in the first months of their service. \n",
    "    At first glance, the negative relationship between revenue and churndep seems like a consequence of negative relationship between loyalty (or frequency of use) and churn behavior. To see if this was the case, 'months' was also included in the correlation table. \n",
    "    However, from the second column of the table, it can be seen that there is a negative relationship between revenue and months, and there is a positive relationship between churn and months. \n",
    "    Of course causality cannot be inferred from a simple analysis like this one, but this reveals the relationship between months and churn. The reason that revenue and churndep have a negative relationship is; the monthly revenue actually decreases with time. On the other hand, people are much less likely to churn in the short term than they are in the long term.(This is explained in the 'eqpdays' part of this answer.)\n",
    "    \n",
    "\n",
    "2 - The linear relationship between mean number of outbound voice calls and churn is also negative.  \n",
    "\n",
    "    The more frequently a customer uses our call service, the less likely he/she is to leave the operator. To prevent the less frequent users from leaving the operator, we can start a campaign \n",
    "    \n",
    "3 - The linear relationship between the number of days the customer has had his/her current equipment is positive. \n",
    "    \n",
    "    The customers who purchase devices with service agreements with our operator have an incentive to remain as our customer until the end of the contract. \n",
    "    In order to prevent churn, we can target the customers whose contract terms are ending soon. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Using the classifier built in 2.3, try predicting `\"churndep\"` on both the train_df and test_df data sets. What is the accuracy on each? If they are different, can you explain the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score of the tree on the training data is 0.999843191369\n",
      "The accuracy score of the tree on the test data is 0.528726\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "## TRAIN DF \n",
    "\n",
    "Y = train_df['churndep']\n",
    "X = train_df.drop('churndep', 1)\n",
    "\n",
    "churn_train_prd = churn_tree.predict(X)\n",
    "# churn_train_prd\n",
    "print ('The accuracy score of the tree on the training data is ' + str(accuracy_score(Y, churn_train_prd)))\n",
    "\n",
    "## TEST DF \n",
    "\n",
    "Z = test_df.drop('churndep', 1)\n",
    "T = test_df['churndep']\n",
    "\n",
    "churn_prd = churn_tree.predict(Z)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print ('The accuracy score of the tree on the test data is ' + '%2f' % (accuracy_score(T, churn_prd)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Explanation__\n",
    "\n",
    "The accuracy scores of the model on training and test datasets are different. \n",
    "\n",
    "The accuracy score on the training data is over 99%. However, the accuracy score on the test data is 52.8%. We see this difference between two datasets because without any limits, our model simply memorizes the training data, and when it encounters new instances it performs poorly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding a Good Decision Tree\n",
    "The default options for your decision tree may not be optimal. We need to analyze whether tuning the parameters can improve the accuracy of the classifier.  For the following options `min_samples_split` and `min_samples_leaf`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Generate a list of 10 values of each for the parameters mim_samples_split and min_samples_leaf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_samples_split_values = np.array(range(300, 3000, 270))\n",
    "min_samples_leaf_values = np.array(range(300, 4000, 370))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Explain in words your reasoning for choosing the above ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, I would grow n trees by n-fold cross validation and test the resulting models against more than one range combination of split-leaf values to find the optimal configuration. Since this was not requested here, I just recalled reading on some articles that people used 1% of the number of total observations as the min_samples_leaf limit and tried that. Since we have sufficient data points, the possible leaf sizes do not seem to be small enough to make the tree overfit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. For each combination of values in 3.1 (there should be 100), build a new classifier and check the classifier's accuracy on the test data. Plot the test set accuracy for these options. Use the values of `min_samples_split` as the x-axis and generate a new series (line) for each of `min_samples_leaf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x11da90b00>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAELCAYAAADz6wBxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nakWFREREakWFREREauX/AeR3+rUxLA2QAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f152550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "## min_leaf values to combine with vstack into an ndarray\n",
    "leaf_to_co = (np.array([min_samples_leaf_values]*10)).reshape(100, 1)\n",
    "## min_split values to combine with vstack into an ndarray\n",
    "split_to_co = (np.array([min_samples_split_values]*10).T).reshape(100,1)\n",
    "\n",
    "## min_leaf and min_split values vstacked to generate combinations of values\n",
    "combine_dt = np.vstack((split_to_co, leaf_to_co))\n",
    "combine_dt = (combine_dt.reshape(2,100)).T\n",
    "\n",
    "## setting the explanatory and target variables from the test data\n",
    "Z = test_df.drop('churndep', 1)\n",
    "T = test_df['churndep']\n",
    "\n",
    "## below these k, l values will be determined by retrieving rows from the vstacked array of \n",
    "## min_leaf and min_split values.\n",
    "def build_tree(k, l):\n",
    "    Comb_Tree = sklearn.tree.DecisionTreeClassifier(criterion='entropy',\n",
    "                                                    min_samples_split=k,\n",
    "                                                    min_samples_leaf=l)\n",
    "    return Comb_Tree\n",
    "\n",
    "## finding the accuracy of the tree that we fit according to the training data on the test data\n",
    "def find_accuracy(Comb_Tree):\n",
    "    Comb_Tree.fit(X, Y)\n",
    "    return accuracy_score(T, Comb_Tree.predict(Z))\n",
    "\n",
    "## creating two empty lists to later fill with the accuracy scores and their corresponding\n",
    "## min_split, min_leaf combinations at the same index\n",
    "accuracy_ls = []\n",
    "combination_ls = []\n",
    "\n",
    "for k, l in combine_dt: \n",
    "    accuracy_ls.append(find_accuracy(build_tree(k,l)))\n",
    "    combination_ls.append([k,l])\n",
    "\n",
    "## Now the lists are filled with combination - accuracy pairs. Preparing them for to\n",
    "## construct a DataFrame.\n",
    "combination_sr = pd.Series(combination_ls)\n",
    "accuracy_sr = pd.Series(accuracy_ls)\n",
    "\n",
    "## From now on, we can use co_ac_df (combination-accuracy df, not so creative, just to remind\n",
    "## the reader of its aim) to find min-max accuracy values and optimal configuration.\n",
    "co_ac_df = (pd.DataFrame(data=[combination_sr, accuracy_sr])).T\n",
    "\n",
    "splits_x  = pd.Series(co_ac_df.iloc[:][0])\n",
    "acc_y = pd.Series(co_ac_df[1][:])\n",
    "\n",
    "x_axis_1 = []\n",
    "for split in splits_x:\n",
    "    x_axis_1.append(split[1])\n",
    "\n",
    "y_axis_1 = []\n",
    "for accuracy in acc_y:\n",
    "    y_axis_1.append(accuracy)\n",
    "\n",
    "plt.plot(x_axis_1[:10], y_axis_1[:10])\n",
    "plt.plot(x_axis_1[10:20], y_axis_1[10:20])\n",
    "plt.plot(x_axis_1[20:30], y_axis_1[20:30])\n",
    "plt.plot(x_axis_1[30:40], y_axis_1[30:40])\n",
    "plt.plot(x_axis_1[40:50], y_axis_1[40:50])\n",
    "plt.plot(x_axis_1[50:60], y_axis_1[50:60])\n",
    "plt.plot(x_axis_1[60:70], y_axis_1[60:70])\n",
    "plt.plot(x_axis_1[70:80], y_axis_1[70:80])\n",
    "plt.plot(x_axis_1[80:90], y_axis_1[80:90])\n",
    "plt.plot(x_axis_1[90:100], y_axis_1[90:100])\n",
    "\n",
    "plt.legend(x_axis_1, loc=1)\n",
    "ax = plt.subplot(111)\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_xlabel('min_samples_split')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Which configuration returns the best accuracy? What is this accuracy? (Note, if you don't see much variation in the test set accuracy across values of min_samples_split or min_samples_leaf, try redoing the above steps with a different range of values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum accuracy is: 0.598721\n",
      "The configuration that yields the best accuracy is: [2190, 300]\n"
     ]
    }
   ],
   "source": [
    "## This is just how the configuration that yields the highest accuracy score is found.\n",
    "## Please see the below cell for explanation.\n",
    "\n",
    "def find_max_conf(DataFrame):\n",
    "    '''Finds the maximum value on the 2nd col of the input DataFrame. \n",
    "    Returns the value of the 1st col at the corresponding row index.'''\n",
    "    max_conf = DataFrame[1].max()\n",
    "    max_co_list = []\n",
    "    for i in range(len(DataFrame[1])):\n",
    "        if DataFrame[1][i] != max_conf:\n",
    "            pass\n",
    "        elif DataFrame[1][i] == max_conf:\n",
    "            max_co_list.append(DataFrame[0][i])\n",
    "            return DataFrame[0][i]\n",
    "    while i not in range(len(DataFrame[1])):\n",
    "        break \n",
    "\n",
    "print ('The maximum accuracy is: ' + str('%2f' % co_ac_df[1].max()))\n",
    "print ('The configuration that yields the best accuracy is: ' + str(find_max_conf(co_ac_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Explanation__\n",
    "\n",
    "    The configuration that yields the best accuracy is: [2190, 300]\n",
    "    \n",
    "The model returns the maximum accuracy when min_samples_split and min_samples_leaf are set to 2190 and 300 respectively. There must be at least 2190 samples on an internal node (a node that can be splitted further), and there must be at least 300 samples on a leaf node. That is, the model can split an internal node to maximum ~7 meaningful leaves/branches. \n",
    "    \n",
    "    The maximum accuracy is: 0.598721\n",
    "    \n",
    "This accuracy score reflects the ( Number of Correct Classifications / Total  Number of Classifications ) ratio. In other words, out of every 100 classifications of our model, maximum 59 classifications will be correct ones. This plain accuracy does not account for the difference between false positives and  false negatives. We can say that approximately 60% is better than just guessing, which can be considered as 50% accuracy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. If you were working for a marketing department, how would you use your churn production model in a real business environment? Explain why churn prediction might be good for the business and how one might improve churn by using this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__\n",
    "\n",
    "If I were working for the marketing department and assuming that I'm not getting access to more meaningful features/models in the future, to target the customers who are more likely to churn I would suggest: \n",
    " \n",
    "__1)__ Arranging a campaign for the customers whose contract terms are ending soon. For example, the telecom company can offer special packages with discounts on the features that the churning users most frequently use. I also assume that this is the data collected from a developed country, so telecom can be considered as a saturated market with a few companies. Therefore these shouldn't be massive discounts that would encourage the company's few big competitors to cut prices just to capture market share.  \n",
    "    \n",
    "__2)__ Besides price discounts, I would also suggest designing a promotional plan for churning customers when we have access to the demographic data for these records, i.e. when we know where to find these customers who are likely to churn. This promotional plan should aim to 1) prevent the customers whose contract terms are ending from churning, 2) if possible, persuade the customers of other companies to switch to our service, because if our customers are churning, in a competitive market like this, the other companies should be losing some customers, too. This includes partnering up with Apple/Samsung/whoever is the most popular smartphone seller in that market and offering advantageous one/two-year smartphone purchase agreements (I don't know if there is a legal max contract term in the US). \n",
    "    \n",
    "__3)__ I would identify the customers whose accounts have a decreasing volume of outbound calls, but I have to say that I don't believe that number of calls itself is an appropriate metric for measuring usage among telecom customers anymore. If I had access to weekly data usage, or even better weekly data usage for social media/messaging packages WITH the number of outbound calls, I believe that would be much more helpful than trying to classify churning customers just according to calls. \n",
    "    \n",
    "__4)__ If I were provided with the data(internet) usage information, we could go back to step 1, and see if we can remix the packages of churning customers according to their needs instead of just offering discounts. Because if we are trying to sell 'minutes/calls' to a customer who in fact does not use them, they wouldn't buy the package until we offer it for free. \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
